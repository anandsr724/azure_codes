{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import functools\n",
    "import time\n",
    "import json\n",
    "from typing import List, Optional, Dict, Any, Callable\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "import requests\n",
    "\n",
    "\n",
    "class KafkaOAuthClient:\n",
    "    \"\"\"\n",
    "    Handles OAuth authentication for Kafka connections.\n",
    "    This is a separate class to avoid code duplication between producer and consumer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_id: str, client_secret: str, token_url: str, scopes: List[str]):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.token_url = token_url\n",
    "        self.scopes = scopes\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_token(self, config: Dict[str, Any]) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieves OAuth access token using client credentials grant.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary (not used but required by Kafka)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (access_token, expiration_timestamp)\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'scope': ' '.join(self.scopes)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                self.token_url,\n",
    "                auth=(self.client_id, self.client_secret),\n",
    "                data=payload\n",
    "            )\n",
    "            resp.raise_for_status()  # Raise exception for bad status codes\n",
    "            \n",
    "            token = resp.json()\n",
    "            self.logger.info(f\"Successfully obtained OAuth token\")\n",
    "            \n",
    "            return token['access_token'], time.time() + float(token['expires_in'])\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"Failed to obtain OAuth token: {e}\")\n",
    "            raise\n",
    "        except KeyError as e:\n",
    "            self.logger.error(f\"Invalid token response format: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class KafkaProducer:\n",
    "    \"\"\"\n",
    "    Kafka Producer with OAuth authentication.\n",
    "    \n",
    "    This class handles sending messages to Kafka topics securely.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers: str, client_id: str, client_secret: str, \n",
    "                 token_url: str, scopes: List[str], ssl_ca_location: str = 'kafka-dev.asd.pem'):\n",
    "        \"\"\"\n",
    "        Initialize Kafka Producer with OAuth authentication.\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Comma-separated list of Kafka brokers\n",
    "            client_id: OAuth client ID\n",
    "            client_secret: OAuth client secret\n",
    "            token_url: OAuth token endpoint URL\n",
    "            scopes: List of OAuth scopes to request\n",
    "            ssl_ca_location: Path to SSL certificate file\n",
    "        \"\"\"\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.ssl_ca_location = ssl_ca_location\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create OAuth client\n",
    "        self.oauth_client = KafkaOAuthClient(client_id, client_secret, token_url, scopes)\n",
    "        \n",
    "        # Create producer with configuration\n",
    "        self.producer = Producer(self._get_producer_config())\n",
    "        self.serializer = StringSerializer('utf8')\n",
    "        \n",
    "        self.logger.info(\"Kafka Producer initialized successfully\")\n",
    "    \n",
    "    def _get_producer_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate producer configuration with OAuth settings.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Producer configuration\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'ssl.ca.location': self.ssl_ca_location,\n",
    "            'oauth_cb': functools.partial(self.oauth_client.get_token),\n",
    "            'logger': self.logger,\n",
    "        }\n",
    "    \n",
    "    def send_message(self, topic: str, message: Any, key: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Send a message to a Kafka topic.\n",
    "        \n",
    "        Args:\n",
    "            topic: Topic name to send message to\n",
    "            message: Message content (will be JSON serialized if not string)\n",
    "            key: Optional message key for partitioning\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert message to JSON string if it's not already a string\n",
    "            if not isinstance(message, str):\n",
    "                message = json.dumps(message)\n",
    "            \n",
    "            # Produce message\n",
    "            self.producer.produce(\n",
    "                topic=topic,\n",
    "                value=message,\n",
    "                key=key,\n",
    "                callback=self._delivery_callback\n",
    "            )\n",
    "            \n",
    "            # Flush to ensure message is sent\n",
    "            self.producer.flush()\n",
    "            \n",
    "            self.logger.info(f\"Message sent to topic '{topic}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to send message to topic '{topic}': {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _delivery_callback(self, err, msg):\n",
    "        \"\"\"\n",
    "        Callback function called when message delivery completes.\n",
    "        \n",
    "        Args:\n",
    "            err: Error object if delivery failed\n",
    "            msg: Message object if delivery succeeded\n",
    "        \"\"\"\n",
    "        if err:\n",
    "            self.logger.error(f\"Message delivery failed: {err}\")\n",
    "        else:\n",
    "            self.logger.debug(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "    \n",
    "    def send_batch(self, topic: str, messages: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Send multiple messages to a topic efficiently.\n",
    "        \n",
    "        Args:\n",
    "            topic: Topic name\n",
    "            messages: List of message dictionaries with 'message' and optional 'key'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for msg_data in messages:\n",
    "                message = msg_data.get('message')\n",
    "                key = msg_data.get('key')\n",
    "                \n",
    "                if not isinstance(message, str):\n",
    "                    message = json.dumps(message)\n",
    "                \n",
    "                self.producer.produce(\n",
    "                    topic=topic,\n",
    "                    value=message,\n",
    "                    key=key,\n",
    "                    callback=self._delivery_callback\n",
    "                )\n",
    "            \n",
    "            # Flush all messages\n",
    "            self.producer.flush()\n",
    "            self.logger.info(f\"Batch of {len(messages)} messages sent to topic '{topic}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to send batch to topic '{topic}': {e}\")\n",
    "            raise\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Clean up producer resources.\"\"\"\n",
    "        if hasattr(self, 'producer'):\n",
    "            self.producer.flush()\n",
    "            self.logger.info(\"Kafka Producer closed\")\n",
    "\n",
    "\n",
    "class KafkaConsumer:\n",
    "    \"\"\"\n",
    "    Kafka Consumer with OAuth authentication.\n",
    "    \n",
    "    This class handles consuming messages from Kafka topics securely.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers: str, client_id: str, client_secret: str,\n",
    "                 token_url: str, scopes: List[str], group_id: str,\n",
    "                 ssl_ca_location: str = 'kafka-dev.asd.pem'):\n",
    "        \"\"\"\n",
    "        Initialize Kafka Consumer with OAuth authentication.\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers: Comma-separated list of Kafka brokers\n",
    "            client_id: OAuth client ID\n",
    "            client_secret: OAuth client secret\n",
    "            token_url: OAuth token endpoint URL\n",
    "            scopes: List of OAuth scopes to request\n",
    "            group_id: Consumer group ID\n",
    "            ssl_ca_location: Path to SSL certificate file\n",
    "        \"\"\"\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.group_id = group_id\n",
    "        self.ssl_ca_location = ssl_ca_location\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create OAuth client\n",
    "        self.oauth_client = KafkaOAuthClient(client_id, client_secret, token_url, scopes)\n",
    "        \n",
    "        # Create consumer with configuration\n",
    "        self.consumer = Consumer(self._get_consumer_config())\n",
    "        \n",
    "        self.logger.info(f\"Kafka Consumer initialized with group_id: {group_id}\")\n",
    "    \n",
    "    def _get_consumer_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate consumer configuration with OAuth settings.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Consumer configuration\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'group.id': self.group_id,\n",
    "            'ssl.ca.location': self.ssl_ca_location,\n",
    "            'oauth_cb': functools.partial(self.oauth_client.get_token),\n",
    "            'logger': self.logger,\n",
    "            'auto.offset.reset': 'earliest',  # Start from beginning if no offset\n",
    "        }\n",
    "    \n",
    "    def _print_assignment(self, consumer, partitions):\n",
    "        \"\"\"Callback for partition assignment.\"\"\"\n",
    "        self.logger.info(f\"Assigned partitions: {partitions}\")\n",
    "    \n",
    "    def subscribe(self, topics: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Subscribe to one or more topics.\n",
    "        \n",
    "        Args:\n",
    "            topics: List of topic names to subscribe to\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.consumer.subscribe(topics, on_assign=self._print_assignment)\n",
    "            self.logger.info(f\"Subscribed to topics: {topics}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to subscribe to topics {topics}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def poll_message(self, timeout: float = 1.0) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Poll for a single message.\n",
    "        \n",
    "        Args:\n",
    "            timeout: Timeout in seconds for polling\n",
    "            \n",
    "        Returns:\n",
    "            Dict with message data or None if no message\n",
    "        \"\"\"\n",
    "        try:\n",
    "            msg = self.consumer.poll(timeout)\n",
    "            \n",
    "            if msg is None:\n",
    "                return None\n",
    "            \n",
    "            if msg.error():\n",
    "                self.logger.error(f\"Consumer error: {msg.error()}\")\n",
    "                return None\n",
    "            \n",
    "            # Return message data\n",
    "            return {\n",
    "                'topic': msg.topic(),\n",
    "                'partition': msg.partition(),\n",
    "                'offset': msg.offset(),\n",
    "                'key': msg.key().decode('utf-8') if msg.key() else None,\n",
    "                'value': msg.value().decode('utf-8') if msg.value() else None,\n",
    "                'timestamp': msg.timestamp()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error polling message: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def consume_messages(self, message_handler: Callable[[Dict[str, Any]], None], \n",
    "                        timeout: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Continuously consume messages and process them with a handler function.\n",
    "        \n",
    "        Args:\n",
    "            message_handler: Function to process each message\n",
    "            timeout: Timeout for each poll operation\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting message consumption...\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                message = self.poll_message(timeout)\n",
    "                \n",
    "                if message:\n",
    "                    try:\n",
    "                        message_handler(message)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error in message handler: {e}\")\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"Consumption interrupted by user\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in message consumption: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.close()\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Clean up consumer resources.\"\"\"\n",
    "        if hasattr(self, 'consumer'):\n",
    "            self.consumer.close()\n",
    "            self.logger.info(\"Kafka Consumer closed\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'bootstrap_servers': 'your-kafka-broker:9092',\n",
    "        'client_id': 'your-client-id',\n",
    "        'client_secret': 'your-client-secret',\n",
    "        'token_url': 'https://your-oauth-server/token',\n",
    "        'scopes': ['kafka.read', 'kafka.write'],\n",
    "        'group_id': 'my-consumer-group'\n",
    "    }\n",
    "    \n",
    "    # Example: Producer usage\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=config['bootstrap_servers'],\n",
    "        client_id=config['client_id'],\n",
    "        client_secret=config['client_secret'],\n",
    "        token_url=config['token_url'],\n",
    "        scopes=config['scopes']\n",
    "    )\n",
    "    \n",
    "    # Send a message\n",
    "    producer.send_message('my-topic', {'message': 'Hello, Kafka!'})\n",
    "    producer.close()\n",
    "    \n",
    "    # Example: Consumer usage\n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=config['bootstrap_servers'],\n",
    "        client_id=config['client_id'],\n",
    "        client_secret=config['client_secret'],\n",
    "        token_url=config['token_url'],\n",
    "        scopes=config['scopes'],\n",
    "        group_id=config['group_id']\n",
    "    )\n",
    "    \n",
    "    # Subscribe to topics\n",
    "    consumer.subscribe(['my-topic'])\n",
    "    \n",
    "    # Define message handler\n",
    "    def handle_message(message):\n",
    "        print(f\"Received: {message['value']} from topic: {message['topic']}\")\n",
    "    \n",
    "    # Start consuming\n",
    "    consumer.consume_messages(handle_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d405e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import functools\n",
    "import json\n",
    "import time\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "import requests\n",
    "\n",
    "class KafkaProducer:\n",
    "    \"\"\"\n",
    "    A class to produce (write) messages to Kafka topics.\n",
    "    Designed for insurance data engineering workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers, client_id, client_secret, token_url, topic_name):\n",
    "        \"\"\"\n",
    "        Initialize the Kafka producer.\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers (str): Kafka server addresses\n",
    "            client_id (str): OAuth client ID for authentication\n",
    "            client_secret (str): OAuth client secret\n",
    "            token_url (str): URL to get OAuth tokens\n",
    "            topic_name (str): Default topic to write to\n",
    "        \"\"\"\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.token_url = token_url\n",
    "        self.topic_name = topic_name\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create producer with configuration\n",
    "        self.producer = Producer(self._get_producer_config())\n",
    "        self.serializer = StringSerializer('utf8')\n",
    "    \n",
    "    def _get_token(self, config):\n",
    "        \"\"\"Get OAuth token for authentication.\"\"\"\n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'scope': 'kafka'  # Adjust scope as needed\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(\n",
    "            self.token_url,\n",
    "            auth=(self.client_id, self.client_secret),\n",
    "            data=payload\n",
    "        )\n",
    "        \n",
    "        token = resp.json()\n",
    "        return token['access_token'], time.time() + float(token['expires_in'])\n",
    "    \n",
    "    def _get_producer_config(self):\n",
    "        \"\"\"Get producer configuration.\"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'ssl.ca.location': 'kafka-dev.asd.pem',\n",
    "            'oauth_cb': functools.partial(self._get_token),\n",
    "            'logger': self.logger,\n",
    "        }\n",
    "    \n",
    "    def send_message(self, message, topic_name=None):\n",
    "        \"\"\"\n",
    "        Send a message to the specified topic.\n",
    "        \n",
    "        Args:\n",
    "            message (dict or str): Message to send\n",
    "            topic_name (str): Topic to send to (uses default if None)\n",
    "        \"\"\"\n",
    "        target_topic = topic_name or self.topic_name\n",
    "        \n",
    "        # Convert message to JSON string if it's a dict\n",
    "        if isinstance(message, dict):\n",
    "            message = json.dumps(message)\n",
    "        \n",
    "        try:\n",
    "            self.producer.produce(target_topic, value=message)\n",
    "            self.producer.flush()  # Ensure message is sent\n",
    "            self.logger.info(f\"Message sent to topic '{target_topic}': {message}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to send message: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_batch(self, messages, topic_name=None):\n",
    "        \"\"\"\n",
    "        Send multiple messages in batch.\n",
    "        \n",
    "        Args:\n",
    "            messages (list): List of messages to send\n",
    "            topic_name (str): Topic to send to\n",
    "        \"\"\"\n",
    "        target_topic = topic_name or self.topic_name\n",
    "        successful_sends = 0\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, dict):\n",
    "                message = json.dumps(message)\n",
    "            \n",
    "            try:\n",
    "                self.producer.produce(target_topic, value=message)\n",
    "                successful_sends += 1\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to send message in batch: {e}\")\n",
    "        \n",
    "        self.producer.flush()\n",
    "        self.logger.info(f\"Sent {successful_sends}/{len(messages)} messages to '{target_topic}'\")\n",
    "        return successful_sends\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the producer connection.\"\"\"\n",
    "        self.producer.flush()\n",
    "        self.logger.info(\"Producer closed\")\n",
    "\n",
    "\n",
    "class KafkaConsumer:\n",
    "    \"\"\"\n",
    "    A class to consume (read) messages from Kafka topics.\n",
    "    Designed for insurance data engineering workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers, client_id, client_secret, token_url, topic_name, group_id=\"insurance-data-eng\"):\n",
    "        \"\"\"\n",
    "        Initialize the Kafka consumer.\n",
    "        \n",
    "        Args:\n",
    "            bootstrap_servers (str): Kafka server addresses\n",
    "            client_id (str): OAuth client ID for authentication\n",
    "            client_secret (str): OAuth client secret\n",
    "            token_url (str): URL to get OAuth tokens\n",
    "            topic_name (str): Topic to read from\n",
    "            group_id (str): Consumer group ID\n",
    "        \"\"\"\n",
    "        self.bootstrap_servers = bootstrap_servers\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.token_url = token_url\n",
    "        self.topic_name = topic_name\n",
    "        self.group_id = group_id\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create consumer with configuration\n",
    "        self.consumer = Consumer(self._get_consumer_config())\n",
    "        self.consumer.subscribe([topic_name], on_assign=self._print_assignment)\n",
    "    \n",
    "    def _get_token(self, config):\n",
    "        \"\"\"Get OAuth token for authentication.\"\"\"\n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'scope': 'kafka'\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(\n",
    "            self.token_url,\n",
    "            auth=(self.client_id, self.client_secret),\n",
    "            data=payload\n",
    "        )\n",
    "        \n",
    "        token = resp.json()\n",
    "        return token['access_token'], time.time() + float(token['expires_in'])\n",
    "    \n",
    "    def _get_consumer_config(self):\n",
    "        \"\"\"Get consumer configuration.\"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'group.id': self.group_id,\n",
    "            'ssl.ca.location': 'kafka-dev.asd.pem',\n",
    "            'oauth_cb': functools.partial(self._get_token),\n",
    "            'logger': self.logger,\n",
    "        }\n",
    "    \n",
    "    def _print_assignment(self, consumer, partitions):\n",
    "        \"\"\"Callback for partition assignment.\"\"\"\n",
    "        self.logger.info(f\"Assignment: {partitions}\")\n",
    "    \n",
    "    def consume_messages(self, timeout=1.0, max_messages=None):\n",
    "        \"\"\"\n",
    "        Consume messages from the topic.\n",
    "        \n",
    "        Args:\n",
    "            timeout (float): Poll timeout in seconds\n",
    "            max_messages (int): Maximum number of messages to consume (None for unlimited)\n",
    "        \n",
    "        Returns:\n",
    "            list: List of consumed messages\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        message_count = 0\n",
    "        \n",
    "        self.logger.info(f\"Starting to consume from topic '{self.topic_name}'\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(timeout)\n",
    "                \n",
    "                if msg is None:\n",
    "                    continue\n",
    "                \n",
    "                if msg.error():\n",
    "                    self.logger.error(f\"Consumer error: {msg.error()}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process the message\n",
    "                message_value = msg.value().decode('utf-8')\n",
    "                try:\n",
    "                    # Try to parse as JSON\n",
    "                    parsed_message = json.loads(message_value)\n",
    "                    messages.append(parsed_message)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If not JSON, store as string\n",
    "                    messages.append(message_value)\n",
    "                \n",
    "                message_count += 1\n",
    "                self.logger.info(f\"Consumed message {message_count}: {message_value}\")\n",
    "                \n",
    "                # Check if we've reached the maximum number of messages\n",
    "                if max_messages and message_count >= max_messages:\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"Consumption interrupted by user\")\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def consume_single_message(self, timeout=5.0):\n",
    "        \"\"\"\n",
    "        Consume a single message from the topic.\n",
    "        \n",
    "        Args:\n",
    "            timeout (float): Poll timeout in seconds\n",
    "        \n",
    "        Returns:\n",
    "            dict or str or None: The consumed message or None if no message\n",
    "        \"\"\"\n",
    "        msg = self.consumer.poll(timeout)\n",
    "        \n",
    "        if msg is None:\n",
    "            return None\n",
    "        \n",
    "        if msg.error():\n",
    "            self.logger.error(f\"Consumer error: {msg.error()}\")\n",
    "            return None\n",
    "        \n",
    "        message_value = msg.value().decode('utf-8')\n",
    "        try:\n",
    "            return json.loads(message_value)\n",
    "        except json.JSONDecodeError:\n",
    "            return message_value\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the consumer connection.\"\"\"\n",
    "        self.consumer.close()\n",
    "        self.logger.info(\"Consumer closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d22b1",
   "metadata": {},
   "source": [
    "Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfba0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import functools\n",
    "import json\n",
    "import time\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "import requests\n",
    "\n",
    "class KafkaConfigurationService:\n",
    "    \"\"\"\n",
    "    Central configuration service that manages environment-specific Kafka configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._configurations = {\n",
    "            \"nonprod\": {\n",
    "                \"bootstrap-servers\": \"nonprod-kafka-broker1:9092,nonprod-kafka-broker2:9092\",\n",
    "                \"client-id-key\": \"nonprod-kafka-client-id\",\n",
    "                \"client-secret-key\": \"nonprod-kafka-client-secret\",\n",
    "                \"token-url\": \"https://nonprod-auth.company.com/oauth/token\",\n",
    "                \"default-secret-scope\": \"nonprod-kafka-secrets\",\n",
    "                \"ssl-ca-location\": \"kafka-nonprod.pem\",\n",
    "                \"default-group-id\": \"nonprod-insurance-data-eng\"\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"bootstrap-servers\": \"prod-kafka-broker1:9092,prod-kafka-broker2:9092\",\n",
    "                \"client-id-key\": \"prod-kafka-client-id\", \n",
    "                \"client-secret-key\": \"prod-kafka-client-secret\",\n",
    "                \"token-url\": \"https://prod-auth.company.com/oauth/token\",\n",
    "                \"default-secret-scope\": \"prod-kafka-secrets\",\n",
    "                \"ssl-ca-location\": \"kafka-prod.pem\",\n",
    "                \"default-group-id\": \"prod-insurance-data-eng\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_configuration(self, environment):\n",
    "        \"\"\"\n",
    "        Get configuration dictionary for a specific environment\n",
    "        \n",
    "        Args:\n",
    "            environment (str): Environment name ('prod' or 'nonprod')\n",
    "            \n",
    "        Returns:\n",
    "            dict: Configuration dictionary for the environment\n",
    "        \"\"\"\n",
    "        if environment not in self._configurations:\n",
    "            raise ValueError(f\"Environment '{environment}' not found in configurations\")\n",
    "        \n",
    "        return self._configurations[environment].copy()\n",
    "    \n",
    "    def add_environment(self, environment, config_dict):\n",
    "        \"\"\"\n",
    "        Add a new environment configuration\n",
    "        \n",
    "        Args:\n",
    "            environment (str): Environment name\n",
    "            config_dict (dict): Configuration dictionary\n",
    "        \"\"\"\n",
    "        self._configurations[environment] = config_dict\n",
    "\n",
    "class KafkaProducer:\n",
    "    \"\"\"\n",
    "    A class to produce (write) messages to Kafka topics.\n",
    "    Designed for insurance data engineering workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, environment='nonprod', topic_name=None, config_service=None, custom_secret_scope=None):\n",
    "        \"\"\"\n",
    "        Initialize the Kafka producer.\n",
    "        \n",
    "        Args:\n",
    "            environment (str): 'prod' or 'nonprod' to determine which config to use\n",
    "            topic_name (str): Default topic to write to\n",
    "            config_service (KafkaConfigurationService): Optional config service instance\n",
    "            custom_secret_scope (str): Optional custom secret scope (uses default if not provided)\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.config_service = config_service or KafkaConfigurationService()\n",
    "        self.env_config = self.config_service.get_configuration(environment)\n",
    "        self.topic_name = topic_name\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Use default secret scope from configuration if not provided\n",
    "        self.secret_scope = custom_secret_scope or self.env_config['default-secret-scope']\n",
    "        \n",
    "        # Get credentials from configuration and secrets\n",
    "        self._setup_credentials()\n",
    "        \n",
    "        # Create producer with configuration\n",
    "        self.producer = Producer(self._get_producer_config())\n",
    "        self.serializer = StringSerializer('utf8')\n",
    "        \n",
    "        print(f\"Kafka Producer initialized for {self.environment} environment\")\n",
    "        print(f\"  Bootstrap servers: {self.bootstrap_servers}\")\n",
    "        print(f\"  Using secret scope: {self.secret_scope}\")\n",
    "    \n",
    "    def _setup_credentials(self):\n",
    "        \"\"\"Setup credentials from environment configuration and secrets.\"\"\"\n",
    "        try:\n",
    "            # Get credentials from Databricks secrets using configuration\n",
    "            self.client_id = dbutils.secrets.get(self.secret_scope, self.env_config['client-id-key'])\n",
    "            self.client_secret = dbutils.secrets.get(self.secret_scope, self.env_config['client-secret-key'])\n",
    "            \n",
    "            # Get other config values\n",
    "            self.bootstrap_servers = self.env_config['bootstrap-servers']\n",
    "            self.token_url = self.env_config['token-url']\n",
    "            self.ssl_ca_location = self.env_config['ssl-ca-location']\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to retrieve credentials from scope '{self.secret_scope}': {str(e)}\")\n",
    "    \n",
    "    def _get_token(self, config):\n",
    "        \"\"\"Get OAuth token for authentication.\"\"\"\n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'scope': 'kafka'  # Adjust scope as needed\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(\n",
    "            self.token_url,\n",
    "            auth=(self.client_id, self.client_secret),\n",
    "            data=payload\n",
    "        )\n",
    "        \n",
    "        token = resp.json()\n",
    "        return token['access_token'], time.time() + float(token['expires_in'])\n",
    "    \n",
    "    def _get_producer_config(self):\n",
    "        \"\"\"Get producer configuration.\"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'ssl.ca.location': self.ssl_ca_location,\n",
    "            'oauth_cb': functools.partial(self._get_token),\n",
    "            'logger': self.logger,\n",
    "        }\n",
    "    \n",
    "    def send_message(self, message, topic_name=None):\n",
    "        \"\"\"\n",
    "        Send a message to the specified topic.\n",
    "        \n",
    "        Args:\n",
    "            message (dict or str): Message to send\n",
    "            topic_name (str): Topic to send to (uses default if None)\n",
    "        \"\"\"\n",
    "        if not topic_name and not self.topic_name:\n",
    "            raise Exception(\"No topic specified. Provide topic_name parameter or set default topic.\")\n",
    "            \n",
    "        target_topic = topic_name or self.topic_name\n",
    "        \n",
    "        # Convert message to JSON string if it's a dict\n",
    "        if isinstance(message, dict):\n",
    "            message = json.dumps(message)\n",
    "        \n",
    "        try:\n",
    "            self.producer.produce(target_topic, value=message)\n",
    "            self.producer.flush()  # Ensure message is sent\n",
    "            self.logger.info(f\"Message sent to topic '{target_topic}': {message}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to send message: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_batch(self, messages, topic_name=None):\n",
    "        \"\"\"\n",
    "        Send multiple messages in batch.\n",
    "        \n",
    "        Args:\n",
    "            messages (list): List of messages to send\n",
    "            topic_name (str): Topic to send to\n",
    "        \"\"\"\n",
    "        if not topic_name and not self.topic_name:\n",
    "            raise Exception(\"No topic specified. Provide topic_name parameter or set default topic.\")\n",
    "            \n",
    "        target_topic = topic_name or self.topic_name\n",
    "        successful_sends = 0\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, dict):\n",
    "                message = json.dumps(message)\n",
    "            \n",
    "            try:\n",
    "                self.producer.produce(target_topic, value=message)\n",
    "                successful_sends += 1\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to send message in batch: {e}\")\n",
    "        \n",
    "        self.producer.flush()\n",
    "        self.logger.info(f\"Sent {successful_sends}/{len(messages)} messages to '{target_topic}'\")\n",
    "        return successful_sends\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the producer connection.\"\"\"\n",
    "        self.producer.flush()\n",
    "        self.logger.info(\"Producer closed\")\n",
    "\n",
    "\n",
    "class KafkaConsumer:\n",
    "    \"\"\"\n",
    "    A class to consume (read) messages from Kafka topics.\n",
    "    Designed for insurance data engineering workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, environment='nonprod', topic_name=None, group_id=None, config_service=None, custom_secret_scope=None):\n",
    "        \"\"\"\n",
    "        Initialize the Kafka consumer.\n",
    "        \n",
    "        Args:\n",
    "            environment (str): 'prod' or 'nonprod' to determine which config to use\n",
    "            topic_name (str): Topic to read from\n",
    "            group_id (str): Consumer group ID (uses default if not provided)\n",
    "            config_service (KafkaConfigurationService): Optional config service instance\n",
    "            custom_secret_scope (str): Optional custom secret scope (uses default if not provided)\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.config_service = config_service or KafkaConfigurationService()\n",
    "        self.env_config = self.config_service.get_configuration(environment)\n",
    "        self.topic_name = topic_name\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Use default secret scope and group ID from configuration if not provided\n",
    "        self.secret_scope = custom_secret_scope or self.env_config['default-secret-scope']\n",
    "        self.group_id = group_id or self.env_config['default-group-id']\n",
    "        \n",
    "        # Get credentials from configuration and secrets\n",
    "        self._setup_credentials()\n",
    "        \n",
    "        # Create consumer with configuration\n",
    "        self.consumer = Consumer(self._get_consumer_config())\n",
    "        \n",
    "        if self.topic_name:\n",
    "            self.consumer.subscribe([self.topic_name], on_assign=self._print_assignment)\n",
    "        \n",
    "        print(f\"Kafka Consumer initialized for {self.environment} environment\")\n",
    "        print(f\"  Bootstrap servers: {self.bootstrap_servers}\")\n",
    "        print(f\"  Group ID: {self.group_id}\")\n",
    "        print(f\"  Using secret scope: {self.secret_scope}\")\n",
    "    \n",
    "    def _setup_credentials(self):\n",
    "        \"\"\"Setup credentials from environment configuration and secrets.\"\"\"\n",
    "        try:\n",
    "            # Get credentials from Databricks secrets using configuration\n",
    "            self.client_id = dbutils.secrets.get(self.secret_scope, self.env_config['client-id-key'])\n",
    "            self.client_secret = dbutils.secrets.get(self.secret_scope, self.env_config['client-secret-key'])\n",
    "            \n",
    "            # Get other config values\n",
    "            self.bootstrap_servers = self.env_config['bootstrap-servers']\n",
    "            self.token_url = self.env_config['token-url']\n",
    "            self.ssl_ca_location = self.env_config['ssl-ca-location']\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to retrieve credentials from scope '{self.secret_scope}': {str(e)}\")\n",
    "    \n",
    "    def _get_token(self, config):\n",
    "        \"\"\"Get OAuth token for authentication.\"\"\"\n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'scope': 'kafka'\n",
    "        }\n",
    "        \n",
    "        resp = requests.post(\n",
    "            self.token_url,\n",
    "            auth=(self.client_id, self.client_secret),\n",
    "            data=payload\n",
    "        )\n",
    "        \n",
    "        token = resp.json()\n",
    "        return token['access_token'], time.time() + float(token['expires_in'])\n",
    "    \n",
    "    def _get_consumer_config(self):\n",
    "        \"\"\"Get consumer configuration.\"\"\"\n",
    "        return {\n",
    "            'bootstrap.servers': self.bootstrap_servers,\n",
    "            'security.protocol': 'sasl_ssl',\n",
    "            'sasl.mechanisms': 'OAUTHBEARER',\n",
    "            'group.id': self.group_id,\n",
    "            'ssl.ca.location': self.ssl_ca_location,\n",
    "            'oauth_cb': functools.partial(self._get_token),\n",
    "            'logger': self.logger,\n",
    "        }\n",
    "    \n",
    "    def _print_assignment(self, consumer, partitions):\n",
    "        \"\"\"Callback for partition assignment.\"\"\"\n",
    "        self.logger.info(f\"Assignment: {partitions}\")\n",
    "    \n",
    "    def subscribe_to_topic(self, topic_name):\n",
    "        \"\"\"\n",
    "        Subscribe to a specific topic.\n",
    "        \n",
    "        Args:\n",
    "            topic_name (str): Topic to subscribe to\n",
    "        \"\"\"\n",
    "        self.topic_name = topic_name\n",
    "        self.consumer.subscribe([topic_name], on_assign=self._print_assignment)\n",
    "        self.logger.info(f\"Subscribed to topic: {topic_name}\")\n",
    "    \n",
    "    def consume_messages(self, timeout=1.0, max_messages=None):\n",
    "        \"\"\"\n",
    "        Consume messages from the topic.\n",
    "        \n",
    "        Args:\n",
    "            timeout (float): Poll timeout in seconds\n",
    "            max_messages (int): Maximum number of messages to consume (None for unlimited)\n",
    "        \n",
    "        Returns:\n",
    "            list: List of consumed messages\n",
    "        \"\"\"\n",
    "        if not self.topic_name:\n",
    "            raise Exception(\"No topic subscribed. Call subscribe_to_topic() first or provide topic in constructor.\")\n",
    "            \n",
    "        messages = []\n",
    "        message_count = 0\n",
    "        \n",
    "        self.logger.info(f\"Starting to consume from topic '{self.topic_name}'\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(timeout)\n",
    "                \n",
    "                if msg is None:\n",
    "                    continue\n",
    "                \n",
    "                if msg.error():\n",
    "                    self.logger.error(f\"Consumer error: {msg.error()}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process the message\n",
    "                message_value = msg.value().decode('utf-8')\n",
    "                try:\n",
    "                    # Try to parse as JSON\n",
    "                    parsed_message = json.loads(message_value)\n",
    "                    messages.append(parsed_message)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If not JSON, store as string\n",
    "                    messages.append(message_value)\n",
    "                \n",
    "                message_count += 1\n",
    "                self.logger.info(f\"Consumed message {message_count}: {message_value}\")\n",
    "                \n",
    "                # Check if we've reached the maximum number of messages\n",
    "                if max_messages and message_count >= max_messages:\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            self.logger.info(\"Consumption interrupted by user\")\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def consume_single_message(self, timeout=5.0):\n",
    "        \"\"\"\n",
    "        Consume a single message from the topic.\n",
    "        \n",
    "        Args:\n",
    "            timeout (float): Poll timeout in seconds\n",
    "        \n",
    "        Returns:\n",
    "            dict or str or None: The consumed message or None if no message\n",
    "        \"\"\"\n",
    "        if not self.topic_name:\n",
    "            raise Exception(\"No topic subscribed. Call subscribe_to_topic() first or provide topic in constructor.\")\n",
    "            \n",
    "        msg = self.consumer.poll(timeout)\n",
    "        \n",
    "        if msg is None:\n",
    "            return None\n",
    "        \n",
    "        if msg.error():\n",
    "            self.logger.error(f\"Consumer error: {msg.error()}\")\n",
    "            return None\n",
    "        \n",
    "        message_value = msg.value().decode('utf-8')\n",
    "        try:\n",
    "            return json.loads(message_value)\n",
    "        except json.JSONDecodeError:\n",
    "            return message_value\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the consumer connection.\"\"\"\n",
    "        self.consumer.close()\n",
    "        self.logger.info(\"Consumer closed\")\n",
    "\n",
    "\n",
    "# Helper functions similar to ADLS connector\n",
    "def create_kafka_producer(environment, topic_name, config_service=None, custom_secret_scope=None):\n",
    "    \"\"\"\n",
    "    Quick helper function to create a Kafka producer\n",
    "    \n",
    "    Args:\n",
    "        environment (str): 'prod' or 'nonprod'\n",
    "        topic_name (str): Default topic name\n",
    "        config_service (KafkaConfigurationService): Optional config service instance\n",
    "        custom_secret_scope (str): Optional custom secret scope\n",
    "    \n",
    "    Returns:\n",
    "        KafkaProducer: Configured producer\n",
    "    \"\"\"\n",
    "    return KafkaProducer(environment, topic_name, config_service, custom_secret_scope)\n",
    "\n",
    "\n",
    "def create_kafka_consumer(environment, topic_name, group_id=None, config_service=None, custom_secret_scope=None):\n",
    "    \"\"\"\n",
    "    Quick helper function to create a Kafka consumer\n",
    "    \n",
    "    Args:\n",
    "        environment (str): 'prod' or 'nonprod'\n",
    "        topic_name (str): Topic to subscribe to\n",
    "        group_id (str): Consumer group ID\n",
    "        config_service (KafkaConfigurationService): Optional config service instance\n",
    "        custom_secret_scope (str): Optional custom secret scope\n",
    "    \n",
    "    Returns:\n",
    "        KafkaConsumer: Configured consumer\n",
    "    \"\"\"\n",
    "    return KafkaConsumer(environment, topic_name, group_id, config_service, custom_secret_scope)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
