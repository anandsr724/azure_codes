{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConnection:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def connection_properties(self, environment):\n",
    "        if environment == 'production':\n",
    "            print(prod)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        jdbcurl = \"jdbc:sqlserver://{}:{};database={};authentication=ActiveDirectoryServicePrincipal;encrypt=true;trustServerCertificate=false;HostNameInCertificate=*.database.windows.net;loginTimeout=30;AADSecurePrincipalId={}@{};AADSecurePrincipalSecret={}\".format(\n",
    "            host,\n",
    "            port,\n",
    "            database,\n",
    "            client_id,\n",
    "            tenant_id,\n",
    "            client_secret\n",
    "        )\n",
    "        \n",
    "        connectionProperties = {\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "        }\n",
    "        \n",
    "        return {\"url\": jdbcurl, \"properties\": connectionProperties}\n",
    "    \n",
    "    def read_data_sqldb(self, read_query, environment):\n",
    "        db_connection = self.connection_properties(environment)\n",
    "        try:\n",
    "            print(\"Reading query...\")\n",
    "            df_read_query = spark.read.jdbc(\n",
    "                url=db_connection[\"url\"],\n",
    "                table=f\"({read_query}) AS temp_table\",\n",
    "                properties=db_connection[\"properties\"]\n",
    "            )\n",
    "            return df_read_query\n",
    "        except Exception as exe:\n",
    "            print(\"Running exception\")\n",
    "            error = type(exe).__name__\n",
    "            error_des = str(exe)\n",
    "            print(f\"Error: {error}\")\n",
    "            print(f\"Error Description: {error_des}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "conn = DatabaseConnection()\n",
    "df = conn.read_data_sqldb(\"SELECT 'test-a1' AS name1, 'test-a2' AS name2\", \"nonprod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdde866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_sqldb(self, dataframe, table_name, schema_name, environment, mode=\"append\"):\n",
    "    \"\"\"\n",
    "    Write dataframe to SQL Server table\n",
    "    \n",
    "    Args:\n",
    "        dataframe: Spark DataFrame to write\n",
    "        table_name: Target table name\n",
    "        schema_name: Target schema name  \n",
    "        environment: 'prod' or 'nonprod'\n",
    "        mode: 'append', 'overwrite', 'ignore', 'error'\n",
    "    \"\"\"\n",
    "    db_connection = self.connection_properties(environment)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Writing data to [{schema_name}].[{table_name}]...\")\n",
    "        \n",
    "        # Full table name with schema\n",
    "        full_table_name = f\"[{schema_name}].[{table_name}]\"\n",
    "        \n",
    "        dataframe.write.jdbc(\n",
    "            url=db_connection[\"url\"],\n",
    "            table=full_table_name,\n",
    "            mode=mode,\n",
    "            properties=db_connection[\"properties\"]\n",
    "        )\n",
    "        \n",
    "        print(\"Data written successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as exe:\n",
    "        print(\"Error occurred during write operation\")\n",
    "        error = type(exe).__name__\n",
    "        error_des = str(exe)\n",
    "        print(f\"Error: {error}\")\n",
    "        print(f\"Error Description: {error_des}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8452ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "conn = DatabaseConnection()\n",
    "\n",
    "# Assuming you have a dataframe 'df_to_insert'\n",
    "success = conn.write_data_sqldb(\n",
    "    dataframe=df_to_insert,\n",
    "    table_name=\"MODEL_OUTPUT\",\n",
    "    schema_name=\"dm_db, \n",
    "    environment='nonprod',\n",
    "    mode=\"append\"  # or \"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_sqldb_advanced(self, dataframe, table_name, schema_name, environment, \n",
    "                             mode=\"append\", batch_size=1000, truncate_table=False):\n",
    "    \"\"\"\n",
    "    Advanced write with more control options\n",
    "    \"\"\"\n",
    "    db_connection = self.connection_properties(environment)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Writing {dataframe.count()} rows to [{schema_name}].[{table_name}]...\")\n",
    "        \n",
    "        full_table_name = f\"[{schema_name}].[{table_name}]\"\n",
    "        \n",
    "        # Additional properties for better performance\n",
    "        write_properties = db_connection[\"properties\"].copy()\n",
    "        write_properties.update({\n",
    "            \"batchsize\": str(batch_size),\n",
    "            \"truncate\": str(truncate_table).lower()\n",
    "        })\n",
    "        \n",
    "        dataframe.write.jdbc(\n",
    "            url=db_connection[\"url\"],\n",
    "            table=full_table_name,\n",
    "            mode=mode,\n",
    "            properties=write_properties\n",
    "        )\n",
    "        \n",
    "        print(\"Data written successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as exe:\n",
    "        print(\"Error occurred during write operation\")\n",
    "        error = type(exe).__name__\n",
    "        error_des = str(exe)\n",
    "        print(f\"Error: {error}\")\n",
    "        print(f\"Error Description: {error_des}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df548a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = DatabaseConnection()\n",
    "\n",
    "# Example 1: Append data to existing table\n",
    "success = conn.write_data_sqldb(\n",
    "    dataframe=your_dataframe,\n",
    "    table_name=\"MODEL_OUTPUT\",\n",
    "    schema_name=\"dm_db\",\n",
    "    environment='nonprod',\n",
    "    mode=\"append\"\n",
    ")\n",
    "\n",
    "# Example 2: Overwrite existing data\n",
    "success = conn.write_data_sqldb(\n",
    "    dataframe=your_dataframe,\n",
    "    table_name=\"MODEL_OUTPUT\",\n",
    "    schema_name=\"dm_db\",\n",
    "    environment='nonprod',\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Example 3: Create sample data and insert\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create sample dataframe (adjust schema to match your table)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "sample_data = [(1, \"test1\", 10.5), (2, \"test2\", 20.3)]\n",
    "sample_df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "# Insert sample data\n",
    "success = conn.write_data_sqldb(\n",
    "    dataframe=sample_df,\n",
    "    table_name=\"MODEL_OUTPUT\",\n",
    "    schema_name=\"dm_db\",\n",
    "    environment='nonprod',\n",
    "    mode=\"append\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
